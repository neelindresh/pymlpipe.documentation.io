{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"PyMLpipe PyMLpipe is a Python library for ease Machine Learning Model monitoring and Deployment. Simple Intuative Easy to use Please Find the Full documentation here! Installation Use the package manager pip to install PyMLpipe. pip install pymlpipe or pip3 install pymlpipe Frame Work Supports [X] Scikit-Learn [X] XGBoost [X] LightGBM [X] Pytorch [ ] Tensorflow [ ] Keras Tutorial (Scikit-Learn|XGBoost|LightGBM) Load the python package from pymlpipe.tabular import PyMLPipe Initiate the PyMLPipe class mlp=PyMLPipe() Set an Experiment Name [Optional] -Default experiment name is '0' mlp.set_experiment(\"IrisDataV2\") Set a version [Optional] -Default there is no version mlp.set_version(0.1) Initiate the context manager - This is create a unique ID for each model run. when .run() is used - Automatic unique ID is generated you can also provide runid argument in the .run() this will the use the given runid for next storing. with mlp.run(): Or with mlp.run(runid='mlopstest'): Set a Tag [Optional] by using set_tag() -Default there is no tags mlp.set_tag('tag') Or Set multiple Tags [Optional] by using set_tags() -Default there is no tags mlp.set_tags([\"Classification\",\"test run\",\"logisticRegression\"]) Set Metrics values [Optional] by using log_matric(metric_name,metric_value) -Default there is no metrics This will help in comparing performance of different models and model versions mlp.log_metric(\"Accuracy\", accuracy_score(testy,predictions)) mlp.log_metric(\"Accuracy\", .92) Set multiple Metrics values [Optional] by using log_matrics({metric_name:metric_value}) -Default there is no metrics mlp.log_metrics( { \"Accuracy\": accuracy_score(testy,predictions), \"Precision\": precision_score(testy,predictions,average='macro'), \"Recall\", recall_score(testy,predictions,average='macro'), } ) mlp.log_metrics( { \"Accuracy\": .92, \"Precision\": .87, \"Recall\", .98, } ) Save an artifact [Optional] - You can save training/testing/validation/dev/prod data for monitoring and comparison This will also help in generating DATA SCHEMA register_artifact() -takes 3 arguments name of artifact Pandas Dataframe type of artifact - [training, testing, validation, dev, prod] You can also use register_artifact_with_path() - This will save the artifact from the disk. Path for the file type of artifact - [training, testing, validation, dev, prod] mlp.register_artifact(\"train.csv\", trainx) mlp.register_artifact(\"train.csv\", trainx) Register Model [Optional] - You can register the model. This will help in Quick deployment mlp.scikit_learn.register_model(\"logistic regression\", model) Quick Start (Scikit-Learn|XGBoost|LightGBM) from sklearn.datasets import load_iris import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score #import PyMLPipe from tabular from pymlpipe.tabular import PyMLPipe # Initiate the class mlp=PyMLPipe() # Set experiment name mlp.set_experiment(\"IrisDataV2\") # Set Version name mlp.set_version(0.2) iris_data=load_iris() data=iris_data[\"data\"] target=iris_data[\"target\"] df=pd.DataFrame(data,columns=iris_data[\"feature_names\"]) trainx,testx,trainy,testy=train_test_split(df,target) # to start monitering use mlp.run() with mlp.run(): # set tags mlp.set_tags([\"Classification\",\"test run\",\"logisticRegression\"]) model=LogisticRegression() model.fit(trainx, trainy) predictions=model.predict(testx) # log performace metrics mlp.log_metric(\"Accuracy\", accuracy_score(testy,predictions)) mlp.log_metric(\"Precision\", precision_score(testy,predictions,average='macro')) mlp.log_metric(\"Recall\", recall_score(testy,predictions,average='macro')) mlp.log_metric(\"F1\", f1_score(testy,predictions,average='macro')) # Save train data and test data mlp.register_artifact(\"train\", trainx) mlp.register_artifact(\"test\", testx,artifact_type=\"testing\") # Save the model mlp.scikit_learn.register_model(\"logistic regression\", model) Launch UI To start the UI pymlpipeui or from pymlpipe.pymlpipeUI import start_ui start_ui(host='0.0.0.0', port=8085) Sample UI One Click Deployment -click the deploy button to deploy the model and get a endpoint Send the data to the Prediction end point in the format Each list is a row of data { \"data\":[ [ 5.6, 3.0, 4.5, 1.5 ], [ 5.6, 3.0, 4.5, 1.5 ] ] } Tutorial (Pytorch) The previous methods can be used as it is. New methods are shown below Log continious Metrics .log_metrics_continious(dict)--> dict of metrics \\ logs the metrics in a continious manner for each epoch mlp.log_metrics_continious({ \"accuracy\": .9, \"precision\": .8, \"recall\": .7 }) To register a pytorch model use .pytorch.register_model(modelname, modelobject) this will Save the model in a .pt file as a torch.jit format for serveing and prediction mlp.pytorch.register_model(\"pytorch_example1\", model) To register a pytorch model use .pytorch.register_model_with_runtime(modelname, modelobject, train_data_sample) train_data_sample - is a sample of input data. it can be random numbers but needs tensor dimension This method is preferred as in future releases this models can be then converted to other formats as well ex: \"onnx\", \"hd5\" mlp.pytorch.register_model_with_runtime(\"pytorch_example1\", model, train_x) Quick Start (Pytorch) import torch import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score,f1_score from pymlpipe.tabular import PyMLPipe df=pd.read_csv(\"train.csv\") encoders=[\"area_code\",\"state\",\"international_plan\",\"voice_mail_plan\",\"churn\"] for i in encoders: le=LabelEncoder() df[i]=le.fit_transform(df[i]) trainy=df[\"churn\"] trainx=df[['state', 'account_length', 'area_code', 'international_plan', 'voice_mail_plan', 'number_vmail_messages', 'total_day_minutes', 'total_day_calls', 'total_day_charge', 'total_eve_minutes', 'total_eve_calls', 'total_eve_charge', 'total_night_minutes', 'total_night_calls', 'total_night_charge', 'total_intl_minutes', 'total_intl_calls', 'total_intl_charge', 'number_customer_service_calls']] class Model(torch.nn.Module): def __init__(self,col_size): super().__init__() # using sequencial self.seq=torch.nn.Sequential( torch.nn.Linear(col_size,15), torch.nn.ReLU(), torch.nn.Linear(15,10), torch.nn.ReLU(), torch.nn.Linear(10,1) ) #using torch layers ''' self.linear_layer_1=torch.nn.Linear(col_size,15) self.relu_1=torch.nn.ReLU() self.linear_layer_2=torch.nn.Linear(15,10) self.relu_2=torch.nn.ReLU() self.linear_layer_3=torch.nn.Linear(10,1) ''' def forward(self,x): out=self.seq(x) ''' out=self.relu_1(self.linear_layer_1(x)) out=self.relu_12self.linear_layer_3(out)) out=self.linear_layer_3(out) ''' return torch.sigmoid(out) model=Model(len(trainx.columns)) train_x,test_x,train_y,test_y=train_test_split(trainx,trainy) train_x=torch.from_numpy(train_x.values) train_x=train_x.type(torch.FloatTensor) train_y=torch.from_numpy(train_y.values) train_y=train_y.type(torch.FloatTensor) test_x=torch.from_numpy(test_x.values) test_x=test_x.type(torch.FloatTensor) test_y=torch.from_numpy(test_y.values) test_y=test_y.type(torch.FloatTensor) optimizer=torch.optim.SGD(model.parameters(),lr=0.001) criterion=torch.nn.BCELoss() def validate(model,testx,testy): prediction=model(testx) prediction=torch.where(prediction>.5,1,0) accu=accuracy_score(prediction.detach().numpy(),test_y.unsqueeze(1).detach().numpy()) f1=f1_score(prediction.detach().numpy(),test_y.unsqueeze(1).detach().numpy()) return {\"accuracy\":accu,\"f1\":f1} epochs=100 batch_size=1000 mlp=PyMLPipe() mlp.set_experiment(\"Pytorch\") mlp.set_version(0.2) with mlp.run(): mlp.register_artifact(\"churndata.csv\",df) mlp.log_params({ \"lr\":0.01, \"optimizer\":\"SGD\", \"loss_fuction\":\"BCEloss\" }) for epoch in range(epochs): loss_batch=0 for batch in range(1000,5000,1000): optimizer.zero_grad() train_data=train_x[batch-1000:batch] output=model(train_data) loss=criterion(output,train_y[batch-1000:batch].unsqueeze(1)) loss.backward() optimizer.step() loss_batch+=loss.item() metrics=validate(model,test_x,test_y) metrics[\"loss\"]=loss_batch metrics[\"epoch\"]=epoch mlp.log_metrics_continious(metrics) mlp.pytorch.register_model(\"pytorch_example1\", model) UI for Pytorch Models Sample input for prediction GET REQUEST - to get info for the model - info : Contains model information - request_body : Sample post Request { \"info\": { \"experiment_id\": \"Pytorch\", \"model_deployment_number\": \"51c186ddd125386c\", \"model_mode\": \"non_runtime\", \"model_type\": \"torch\", \"model_url\": \"/predict/51c186ddd125386c\", \"run_id\": \"3fffe458-9676-4bc7-a6c0-a3b4cf38e277\", \"status\": \"running\" }, \"request_body\": { \"data\": [ [ 42.0, 120.0, 1.0, 0.0, 0.0, 0.0, 185.7, 133.0, 31.57, 235.1, 149.0, 19.98, 256.4, 78.0, 11.54, 16.9, 6.0, 4.56, 0.0 ] ], \"dtype\": \"float\" } } For POST REQUEST - data --> list: contains data rows for prediction supports both batch prediction and single instance ex: data --> [ [ 0,1,2,3],[3,4,56 ] ] - dtype --> str: for type conversion converts the data into required data type tensor { \"data\": [ [ 42.0, 120.0, 1.0, 0.0, 0.0, 0.0, 185.7, 133.0, 31.57, 235.1, 149.0, 19.98, 256.4, 78.0, 11.54, 16.9, 6.0, 4.56, 0.0 ] ], \"dtype\": \"float\" } Contributing Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change. Please make sure to update tests as appropriate. License MIT","title":"Home"},{"location":"#pymlpipe","text":"PyMLpipe is a Python library for ease Machine Learning Model monitoring and Deployment. Simple Intuative Easy to use Please Find the Full documentation here!","title":"PyMLpipe"},{"location":"#installation","text":"Use the package manager pip to install PyMLpipe. pip install pymlpipe or pip3 install pymlpipe","title":"Installation"},{"location":"#frame-work-supports","text":"[X] Scikit-Learn [X] XGBoost [X] LightGBM [X] Pytorch [ ] Tensorflow [ ] Keras","title":"Frame Work Supports"},{"location":"#tutorial-scikit-learnxgboostlightgbm","text":"Load the python package from pymlpipe.tabular import PyMLPipe Initiate the PyMLPipe class mlp=PyMLPipe() Set an Experiment Name [Optional] -Default experiment name is '0' mlp.set_experiment(\"IrisDataV2\") Set a version [Optional] -Default there is no version mlp.set_version(0.1) Initiate the context manager - This is create a unique ID for each model run. when .run() is used - Automatic unique ID is generated you can also provide runid argument in the .run() this will the use the given runid for next storing. with mlp.run(): Or with mlp.run(runid='mlopstest'): Set a Tag [Optional] by using set_tag() -Default there is no tags mlp.set_tag('tag') Or Set multiple Tags [Optional] by using set_tags() -Default there is no tags mlp.set_tags([\"Classification\",\"test run\",\"logisticRegression\"]) Set Metrics values [Optional] by using log_matric(metric_name,metric_value) -Default there is no metrics This will help in comparing performance of different models and model versions mlp.log_metric(\"Accuracy\", accuracy_score(testy,predictions)) mlp.log_metric(\"Accuracy\", .92) Set multiple Metrics values [Optional] by using log_matrics({metric_name:metric_value}) -Default there is no metrics mlp.log_metrics( { \"Accuracy\": accuracy_score(testy,predictions), \"Precision\": precision_score(testy,predictions,average='macro'), \"Recall\", recall_score(testy,predictions,average='macro'), } ) mlp.log_metrics( { \"Accuracy\": .92, \"Precision\": .87, \"Recall\", .98, } ) Save an artifact [Optional] - You can save training/testing/validation/dev/prod data for monitoring and comparison This will also help in generating DATA SCHEMA register_artifact() -takes 3 arguments name of artifact Pandas Dataframe type of artifact - [training, testing, validation, dev, prod] You can also use register_artifact_with_path() - This will save the artifact from the disk. Path for the file type of artifact - [training, testing, validation, dev, prod] mlp.register_artifact(\"train.csv\", trainx) mlp.register_artifact(\"train.csv\", trainx) Register Model [Optional] - You can register the model. This will help in Quick deployment mlp.scikit_learn.register_model(\"logistic regression\", model)","title":"Tutorial (Scikit-Learn|XGBoost|LightGBM)"},{"location":"#quick-start-scikit-learnxgboostlightgbm","text":"from sklearn.datasets import load_iris import pandas as pd from sklearn.model_selection import train_test_split from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score #import PyMLPipe from tabular from pymlpipe.tabular import PyMLPipe # Initiate the class mlp=PyMLPipe() # Set experiment name mlp.set_experiment(\"IrisDataV2\") # Set Version name mlp.set_version(0.2) iris_data=load_iris() data=iris_data[\"data\"] target=iris_data[\"target\"] df=pd.DataFrame(data,columns=iris_data[\"feature_names\"]) trainx,testx,trainy,testy=train_test_split(df,target) # to start monitering use mlp.run() with mlp.run(): # set tags mlp.set_tags([\"Classification\",\"test run\",\"logisticRegression\"]) model=LogisticRegression() model.fit(trainx, trainy) predictions=model.predict(testx) # log performace metrics mlp.log_metric(\"Accuracy\", accuracy_score(testy,predictions)) mlp.log_metric(\"Precision\", precision_score(testy,predictions,average='macro')) mlp.log_metric(\"Recall\", recall_score(testy,predictions,average='macro')) mlp.log_metric(\"F1\", f1_score(testy,predictions,average='macro')) # Save train data and test data mlp.register_artifact(\"train\", trainx) mlp.register_artifact(\"test\", testx,artifact_type=\"testing\") # Save the model mlp.scikit_learn.register_model(\"logistic regression\", model)","title":"Quick Start (Scikit-Learn|XGBoost|LightGBM)"},{"location":"#launch-ui","text":"To start the UI pymlpipeui or from pymlpipe.pymlpipeUI import start_ui start_ui(host='0.0.0.0', port=8085)","title":"Launch UI"},{"location":"#sample-ui","text":"","title":"Sample UI"},{"location":"#one-click-deployment-click-the-deploy-button-to-deploy-the-model-and-get-a-endpoint","text":"","title":"One Click Deployment -click the deploy button to deploy the model and get a endpoint"},{"location":"#send-the-data-to-the-prediction-end-point-in-the-format","text":"Each list is a row of data { \"data\":[ [ 5.6, 3.0, 4.5, 1.5 ], [ 5.6, 3.0, 4.5, 1.5 ] ] }","title":"Send the data to the Prediction end point in the format"},{"location":"#tutorial-pytorch","text":"","title":"Tutorial (Pytorch)"},{"location":"#the-previous-methods-can-be-used-as-it-is-new-methods-are-shown-below","text":"Log continious Metrics .log_metrics_continious(dict)--> dict of metrics \\ logs the metrics in a continious manner for each epoch mlp.log_metrics_continious({ \"accuracy\": .9, \"precision\": .8, \"recall\": .7 }) To register a pytorch model use .pytorch.register_model(modelname, modelobject) this will Save the model in a .pt file as a torch.jit format for serveing and prediction mlp.pytorch.register_model(\"pytorch_example1\", model) To register a pytorch model use .pytorch.register_model_with_runtime(modelname, modelobject, train_data_sample) train_data_sample - is a sample of input data. it can be random numbers but needs tensor dimension This method is preferred as in future releases this models can be then converted to other formats as well ex: \"onnx\", \"hd5\" mlp.pytorch.register_model_with_runtime(\"pytorch_example1\", model, train_x)","title":"The previous methods can be used as it is. New methods are shown below"},{"location":"#quick-start-pytorch","text":"import torch import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score,f1_score from pymlpipe.tabular import PyMLPipe df=pd.read_csv(\"train.csv\") encoders=[\"area_code\",\"state\",\"international_plan\",\"voice_mail_plan\",\"churn\"] for i in encoders: le=LabelEncoder() df[i]=le.fit_transform(df[i]) trainy=df[\"churn\"] trainx=df[['state', 'account_length', 'area_code', 'international_plan', 'voice_mail_plan', 'number_vmail_messages', 'total_day_minutes', 'total_day_calls', 'total_day_charge', 'total_eve_minutes', 'total_eve_calls', 'total_eve_charge', 'total_night_minutes', 'total_night_calls', 'total_night_charge', 'total_intl_minutes', 'total_intl_calls', 'total_intl_charge', 'number_customer_service_calls']] class Model(torch.nn.Module): def __init__(self,col_size): super().__init__() # using sequencial self.seq=torch.nn.Sequential( torch.nn.Linear(col_size,15), torch.nn.ReLU(), torch.nn.Linear(15,10), torch.nn.ReLU(), torch.nn.Linear(10,1) ) #using torch layers ''' self.linear_layer_1=torch.nn.Linear(col_size,15) self.relu_1=torch.nn.ReLU() self.linear_layer_2=torch.nn.Linear(15,10) self.relu_2=torch.nn.ReLU() self.linear_layer_3=torch.nn.Linear(10,1) ''' def forward(self,x): out=self.seq(x) ''' out=self.relu_1(self.linear_layer_1(x)) out=self.relu_12self.linear_layer_3(out)) out=self.linear_layer_3(out) ''' return torch.sigmoid(out) model=Model(len(trainx.columns)) train_x,test_x,train_y,test_y=train_test_split(trainx,trainy) train_x=torch.from_numpy(train_x.values) train_x=train_x.type(torch.FloatTensor) train_y=torch.from_numpy(train_y.values) train_y=train_y.type(torch.FloatTensor) test_x=torch.from_numpy(test_x.values) test_x=test_x.type(torch.FloatTensor) test_y=torch.from_numpy(test_y.values) test_y=test_y.type(torch.FloatTensor) optimizer=torch.optim.SGD(model.parameters(),lr=0.001) criterion=torch.nn.BCELoss() def validate(model,testx,testy): prediction=model(testx) prediction=torch.where(prediction>.5,1,0) accu=accuracy_score(prediction.detach().numpy(),test_y.unsqueeze(1).detach().numpy()) f1=f1_score(prediction.detach().numpy(),test_y.unsqueeze(1).detach().numpy()) return {\"accuracy\":accu,\"f1\":f1} epochs=100 batch_size=1000 mlp=PyMLPipe() mlp.set_experiment(\"Pytorch\") mlp.set_version(0.2) with mlp.run(): mlp.register_artifact(\"churndata.csv\",df) mlp.log_params({ \"lr\":0.01, \"optimizer\":\"SGD\", \"loss_fuction\":\"BCEloss\" }) for epoch in range(epochs): loss_batch=0 for batch in range(1000,5000,1000): optimizer.zero_grad() train_data=train_x[batch-1000:batch] output=model(train_data) loss=criterion(output,train_y[batch-1000:batch].unsqueeze(1)) loss.backward() optimizer.step() loss_batch+=loss.item() metrics=validate(model,test_x,test_y) metrics[\"loss\"]=loss_batch metrics[\"epoch\"]=epoch mlp.log_metrics_continious(metrics) mlp.pytorch.register_model(\"pytorch_example1\", model)","title":"Quick Start (Pytorch)"},{"location":"#ui-for-pytorch-models","text":"","title":"UI for Pytorch Models"},{"location":"#sample-input-for-prediction","text":"GET REQUEST - to get info for the model - info : Contains model information - request_body : Sample post Request { \"info\": { \"experiment_id\": \"Pytorch\", \"model_deployment_number\": \"51c186ddd125386c\", \"model_mode\": \"non_runtime\", \"model_type\": \"torch\", \"model_url\": \"/predict/51c186ddd125386c\", \"run_id\": \"3fffe458-9676-4bc7-a6c0-a3b4cf38e277\", \"status\": \"running\" }, \"request_body\": { \"data\": [ [ 42.0, 120.0, 1.0, 0.0, 0.0, 0.0, 185.7, 133.0, 31.57, 235.1, 149.0, 19.98, 256.4, 78.0, 11.54, 16.9, 6.0, 4.56, 0.0 ] ], \"dtype\": \"float\" } } For POST REQUEST - data --> list: contains data rows for prediction supports both batch prediction and single instance ex: data --> [ [ 0,1,2,3],[3,4,56 ] ] - dtype --> str: for type conversion converts the data into required data type tensor { \"data\": [ [ 42.0, 120.0, 1.0, 0.0, 0.0, 0.0, 185.7, 133.0, 31.57, 235.1, 149.0, 19.98, 256.4, 78.0, 11.54, 16.9, 6.0, 4.56, 0.0 ] ], \"dtype\": \"float\" }","title":"Sample input for prediction"},{"location":"#contributing","text":"Pull requests are welcome. For major changes, please open an issue first to discuss what you would like to change. Please make sure to update tests as appropriate.","title":"Contributing"},{"location":"#license","text":"MIT","title":"License"},{"location":"code/","text":"Source Code Index pymlpipe/tabular import os from pymlpipe.utils.database import create_folder from pymlpipe.utils.getschema import schema_ import uuid import yaml from contextlib import contextmanager import pandas as pd import shutil import pickle import sklearn import datetime class Context_Manager: \"\"\"_summary_: Context Manager for with statement 1. creates folders and subfolders 2. creates runid for a run instance \"\"\" def __init__(self, name, feature_store, run_id=None): super(PyMLPipe) if run_id == None: self.runid = str(uuid.uuid4()) else: self.runid = run_id self.name = name self.feature_store = feature_store self.exp_path = os.path.join(self.feature_store, self.name, self.runid) self.folders = {'artifacts': os.path.join(self.exp_path, 'artifacts'), 'metrics': os.path.join(self.exp_path, 'metrics'), 'models': os.path.join(self.exp_path, 'models'), 'params': os.path.join(self.exp_path, 'params')} self.info_dict = [] def get_path(self): \"\"\"_summary_ Returns: _type_: _description_ \"\"\" return self.exp_path def structure(self): \"\"\"_summary_ Returns: _type_: _description_ \"\"\" self.exp_path = create_folder(self.feature_store, self.name) self.exp_path = create_folder(self.exp_path, self.runid) self._create_all_folders(self.exp_path) return self.exp_path def _create_all_folders(self, exp_path): \"\"\"_summary_ Args: exp_path (_type_): _description_ \"\"\" for i in self.folders: create_folder(exp_path, i) def write_to_yaml(self, info): with open(os.path.join(self.exp_path, 'info.yaml'), 'w') as file: documents = yaml.dump(info, file) class PyMLPipe: def __init__(self): self.feature_store = create_folder(os.getcwd()) self.experiment_name = '0' self.folders = None self.experiment_path = None self.info = {} self.info['tags'] = [] self.info['metrics'] = {} self.info['params'] = {} self.info['artifact'] = [] self.info['model'] = {} self.info['artifact_schema'] = [] def __reset__(self): self.feature_store = create_folder(os.getcwd()) self.folders = None self.experiment_path = None self.info['tags'] = [] self.info['metrics'] = {} self.info['params'] = {} self.info['artifact'] = [] self.info['model'] = {} self.info['artifact_schema'] = [] @contextmanager def run(self, experiment_name=None, runid=None): \"\"\"_summary_: start a context manager for with statement 1. When run is started it will create a. RUN ID b. EXPERIMENT ID c. FOLDERS for storing the details Args: experiment_name (str, optional): gives a experiment name. Defaults to None. runid (str, optional): gives a runid. Defaults to None. Returns: class context_run(object): object for the context manager \"\"\" if experiment_name != None: self.experiment_name = experiment_name r = Context_Manager(self.experiment_name, self.feature_store, runid) self._write_info_run(self.experiment_name, r.runid) r.structure() self.context_manager = r self.scikit_learn = ScikitLearn(self.context_manager.folders) yield r self.info['execution_time'] = str(datetime.datetime.now()).split('.')[0] if self.scikit_learn.registered: self.info['model'] = {'model_name': self.scikit_learn.model_name, 'model_path': self.scikit_learn.model_path, 'model_params': self.scikit_learn.model_params, 'model_class': self.scikit_learn.model_class, 'model_type': self.scikit_learn.model_type, 'model_tags': self.scikit_learn.model_tags, 'registered': self.scikit_learn.registered} self.context_manager.write_to_yaml(self.info) self.__reset__() def set_experiment(self, name): \"\"\"_summary_: sets the experiment name Args: name (str): name of the experiment \"\"\" self.experiment_name = name exp_path = create_folder(self.feature_store, self.experiment_name) self._write_info_experiment(name, exp_path) def set_tag(self, tag_value): \"\"\"_summary_: sets a tag for a perticular run Args: name (str or int or float): tag name Raises: TypeError: Supported type 'str','int','float' \"\"\" if isinstance(tag_value, dict) or isinstance(tag_value, list) or isinstance(tag_value, set): raise TypeError(\"unsupported type, Expected 'str','int','float' got \" + str(type(tag_value))) self.info['tags'].append(tag_value) def set_tags(self, tag_dict: list): \"\"\"_summary_:sets N no of tags for a perticular run Args: tag_dict (list): tag names in list format Raises: TypeError: Expected 'list' \"\"\" if isinstance(tag_dict, list): self.info['tags'].extend(tag_dict) else: raise TypeError(\"unsupported type, Expected 'list' got \" + str(type(tag_dict))) def get_tags(self): \"\"\"_summary_: get all the tags that are associated with the run Returns: list: tags that are associated with the run \"\"\" return self.info['tags'] def set_version(self, version): \"\"\"_summary_:sets version number for the perticular run Args: version (str or int or float): version number Raises: TypeError: Expected 'str','int','float' \"\"\" if isinstance(version, dict) or isinstance(version, list) or isinstance(version, set): raise TypeError(\"unsupported type, Expected 'str','int','float' got \" + str(type(tag_dict))) self.info['version'] = version def get_version(self): \"\"\"_summary_:get the version number associated with the run Returns: _type_: version number \"\"\" return self.info['version'] def log_metrics(self, metric_dict: dict): \"\"\"_summary_: log metrics for the model run Args: metric_dict (dict): key value pair with metric name and metric value Raises: TypeError: Expected 'dict' \"\"\" if isinstance(metric_dict, dict): self.info['metrics'].update({i: float('{0:.2f}'.format(j)) for (i, j) in metric_dict.items()}) else: raise TypeError(\"unsupported type, Expected 'dict' got \" + str(type(metric_dict))) def log_metric(self, metric_name, metric_value): \"\"\"_summary_: log single metric for the model run Args: metric_name (str): name of the metric metric_value (int or float): value of the metric Raises: TypeError: metric_name expected to be str TypeError: metric_value expected to be int or float \"\"\" mv = None if not isinstance(metric_value, int) and (not isinstance(metric_value, float)): raise TypeError(\"unsupported type, 'metric_value' Expected 'int','float' got \" + str(type(metric_value))) if not isinstance(metric_name, str): raise TypeError(\"unsupported type, 'metric_value' Expected 'str' got \" + str(type(metric_name))) self.info['metrics'][metric_name] = float('{0:.2f}'.format(metric_value)) def log_params(self, param_dict: dict): \"\"\"_summary_: log parameters for the model run Args: param_dict (dict): key value pair with parameter name and parameter value Raises: TypeError: Expected 'dict' \"\"\" if isinstance(param_dict, dict): self.info['params'].update(param_dict) else: raise TypeError(\"unsupported type, Expected 'dict' got \" + str(type(metric_dict))) def log_param(self, param_name, param_value): \"\"\"_summary_:log single parameter for the model run Args: param_name (str): _description_ param_value (int or float or str): _description_ Raises: TypeError: param_name Expected 'str' TypeError: param_value Expected 'int','float','str' \"\"\" mv = None if not isinstance(param_value, int) and (not isinstance(param_value, float)) and (not isinstance(param_value, str)): raise TypeError(\"unsupported type, 'param_value' Expected 'int','float','str' got \" + str(type(metric_value))) if not isinstance(param_name, str): raise TypeError(\"unsupported type, 'param_name' Expected 'str' got \" + str(type(metric_name))) self.info['params'][param_name] = param_value def register_artifact(self, artifact_name, artifact, artifact_type='training'): \"\"\"_summary_: Save Artifact as part of data verion control Args: artifact_name (str): name of the artifact artifact (pandas DataFrame): pandas DataFrame object with the data artifact_type (str, optional): Defaults to \"training\". artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: Expected DataFrame object ValueError: artifact_name should have a string value \"\"\" if not isinstance(artifact, pd.DataFrame): raise TypeError(\"Please provide DataFrame in 'artifact'\") if artifact_name == '' or artifact_name == None: raise ValueError(\"Please provide a name in 'artifact_name' which is not '' or None\") path = os.path.join(self.context_manager.folders['artifacts'], artifact_name) dataschema = artifact.describe(include='all') artifact.to_csv(path, index=False) self.info['artifact'].append({'name': artifact_name, 'path': path, 'tag': artifact_type}) (schema_data, schema_details) = schema_(artifact) self.info['artifact_schema'].append({'name': artifact_name, 'schema': schema_data, 'details': schema_details}) def register_artifact_with_path(self, artifact, artifact_type='training'): \"\"\"_summary_ Args: artifact (str): path of the artifact artifact_type (str, optional): _description_. Defaults to \"training\".artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: artifact path should be str ValueError: artifact path should be correct \"\"\" if not isinstance(artifact, str): raise TypeError('Please provide full path of artifact') if not os.path.exists(artifact): raise ValueError('Please provide correct path of artifact') shutil.copy(artifact, self.context_manager.folders['artifacts']) path = os.path.join(self.context_manager.folders['artifacts'], os.path.basename(artifact)) self.info['artifact'].append({'name': os.path.basename(path), 'path': path, 'tag': artifact_type}) filename = os.path.basename(artifact) if filename.endswith('.csv'): artifact = pd.read_csv(path) elif filename.endswith('.xlxs'): artifact = pd.read_excel(path) elif filename.endswith('.parquet'): artifact = pd.read_parquet(path) else: print('Error: Unknown file type cannot generate Schema!!!!') return (schema_data, schema_details) = schema_(artifact) self.info['artifact_schema'].append({'name': filename, 'schema': schema_data, 'details': schema_details}) def get_info(self): \"\"\"_summary_: get the whole run details Returns: dict: information about the whole run \"\"\" return self.info def get_artifact(self): \"\"\"_summary_: get the artifact details Returns: dict: returns the artifact detail \"\"\" return self.info['artifact'] def _write_info_experiment(self, experiment_name, path): \"\"\"_summary_: writes to the experiment schema Args: experiment_name (str): name of the experiment path (str): path to save the run details \"\"\" fulllist = {} if os.path.exists(os.path.join(self.feature_store, 'experiment.yaml')): with open(os.path.join(self.feature_store, 'experiment.yaml')) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) if experiment_name not in fulllist: fulllist[experiment_name] = {'experiment_path': path, 'runs': [], 'execution_time': str(datetime.datetime.now()).split('.')[0]} else: fulllist[experiment_name]['execution_time'] = str(datetime.datetime.now()).split('.')[0] else: fulllist[experiment_name] = {'experiment_path': path, 'runs': [], 'execution_time': str(datetime.datetime.now()).split('.')[0]} with open(os.path.join(self.feature_store, 'experiment.yaml'), 'w') as file: documents = yaml.dump(fulllist, file) def _write_info_run(self, experiment_name, run_id): \"\"\"_summary_:writes to the run schema Args: experiment_name (str): name of the experiment run_id (str): ID for the running instance \"\"\" fulllist = {} with open(os.path.join(self.feature_store, 'experiment.yaml')) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) fulllist[experiment_name]['runs'].append(run_id) with open(os.path.join(self.feature_store, 'experiment.yaml'), 'w') as file: documents = yaml.dump(fulllist, file) def set_uri(self): pass class ScikitLearn: def __init__(self, folders): self.folders = folders self.model_name = '' self.model_path = '' self.model_class = '' self.model_type = '' self.model_params = {} self.model_tags = {} self.registered = False def register_model(self, model_name, model): if 'sklearn' in str(type(model)): pickle.dump(model, open(os.path.join(self.folders['models'], model_name + '.pkl'), 'wb')) else: raise TypeError('Error:Expected ScikitLearn Module!!!!') self.model_name = model_name self.model_path = os.path.join(self.folders['models'], model_name + '.pkl') self.model_class = type(model).__name__ self.model_params = model.get_params() self.model_tags = {tag: str(value) for (tag, value) in model._get_tags().items()} self.model_type = 'scikit-learn' self.registered = True pymlpipe/ init pymlpipe/pymlpipeUI import flask import os from pymlpipe.utils import yamlio from pymlpipe.utils import _sklearn_prediction from flask_api import FlaskAPI import numpy as np import json import uuid app = FlaskAPI(__name__) BASE_DIR = os.getcwd() MODEL_FOLDER_NAME = 'modelrun' MODEL_DIR = os.path.join(BASE_DIR, MODEL_FOLDER_NAME) EXPERIMENT_FILE = 'experiment.yaml' DEPLOYMENT_FILE = 'deployment.yaml' PREDICTORS = {} app.secret_key = 'PYMLPIPE_SEC_KEY' @app.route('/') def index(): \"\"\" if \"status\" in flask.request.args: if flask.request.args[\"status\"]==\"501\": deploy_status=False \"\"\" metric_filters = {} tag_filters = [] if len(flask.request.args): if 'metrics' in flask.request.args: metric_filters[flask.request.args['metrics']] = flask.request.args['metricsfilter'] elif 'tags' in flask.request.args: tag_filters = flask.request.args['tags'].split(',') experiment_lists = yamlio.read_yaml(os.path.join(MODEL_DIR, EXPERIMENT_FILE)) if len(experiment_lists) == 0: return flask.render_template('index.html', runs=[], run_details={}, metrics=[], current_experiment=None) info = {} metrics = [] exp_wise_metrics = {} tags = [] error = '' for (experiment, run_data) in experiment_lists.items(): for run_id in run_data['runs']: print(run_data['experiment_path'], run_id, 'info.yaml') run_folder = os.path.join(run_data['experiment_path'], run_id, 'info.yaml') run_details = yamlio.read_yaml(run_folder) info[run_id] = run_details if 'tags' in run_details: tags.extend(run_details['tags']) if 'metrics' in run_details: metrics.extend(list(run_details['metrics'].keys())) exp_wise_metrics[experiment] = list(run_details['metrics'].keys()) info = {run: info[run] for run in info if len(info[run]) > 0} if len(metric_filters) > 0: newinfo = {} for (run_id, details) in info.items(): for mfilter in metric_filters: if mfilter in details['metrics']: fv = details['metrics'][mfilter] try: if eval(str(fv) + metric_filters[mfilter]): newinfo[run_id] = details except Exception as e: error = e else: newinfo[run_id] = details info = newinfo elif len(tag_filters) > 0: newinfo = {} for (run_id, details) in info.items(): if len(set(tag_filters).intersection(set(details['tags']))) > 0: newinfo[run_id] = details info = newinfo exp_names = list(experiment_lists.keys()) return flask.render_template('index.html', runs=experiment_lists, run_details=info, metrics=list(set(metrics)), current_experiment=exp_names, tags=list(set(tags)), exp_wise_metrics=exp_wise_metrics, error=error) @app.route('/run/<run_id>/') def runpage(run_id): deploy_status = True if 'status' in flask.request.args: if flask.request.args['status'] == '501': deploy_status = False (experiments, run_id) = run_id.split('@') experiment_lists = yamlio.read_yaml(os.path.join(MODEL_DIR, EXPERIMENT_FILE)) run_details = yamlio.read_yaml(os.path.join(MODEL_DIR, experiments, run_id, 'info.yaml')) expertiment_details = {'RUN_ID': run_id, 'EXPERIMENT NAME': experiments, 'EXECUTION DATE TIME': run_details['execution_time']} if 'tags' in run_details: expertiment_details['TAGS'] = run_details['tags'] else: expertiment_details['TAGS'] = '-' if 'version' in run_details: expertiment_details['VERSION'] = run_details['version'] else: expertiment_details['VERSION'] = '-' return flask.render_template('run.html', run_id=run_id, experiments=experiments, expertiment_details=expertiment_details, artifact_details=run_details['artifact'], metrics_details=run_details['metrics'], model_details=run_details['model'], param_details=run_details['params'], schema_details=run_details['artifact_schema'], is_deployed=True if 'model_path' in run_details['model'] else False, deploy_status=deploy_status) @app.route('/download_artifact/<uid>') def download_artifact(uid): (experiments, run_id, filename) = uid.split('@') return flask.send_from_directory(os.path.join(MODEL_DIR, experiments, run_id, 'artifacts'), filename, as_attachment=True) @app.route('/download_model/<uid>') def download_model(uid): (experiments, run_id, filename, model_type) = uid.split('@') if model_type == 'scikit-learn': filename = filename + '.pkl' return flask.send_from_directory(os.path.join(MODEL_DIR, experiments, run_id, 'models'), filename, as_attachment=True) @app.route('/deployments/<run_id>/') def deployments(run_id): (experiments, runid) = run_id.split('@') run_details = yamlio.read_yaml(os.path.join(MODEL_DIR, experiments, runid, 'info.yaml')) deployed = _sklearn_prediction.Deployment(run_details['model']['model_path']) print(uuid.NAMESPACE_DNS) run_hash = str(uuid.uuid3(uuid.NAMESPACE_DNS, run_id)).replace('-', '')[:16] if run_hash not in PREDICTORS: PREDICTORS[run_hash] = deployed ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) ALL_DEPLOYED_MODELS.append({'run_id': runid, 'experiment_id': experiments, 'model_path': run_details['model']['model_path'], 'model_deployment_number': run_hash, 'model_url': '/predict/' + run_hash, 'status': 'running'}) yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) return flask.redirect(flask.url_for('show_deployments')) return flask.redirect('/run/' + run_id + '?status=501') @app.route('/show_deployments/') def show_deployments(): ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) return flask.render_template('deployments.html', ALL_DEPLOYED_MODELS=ALL_DEPLOYED_MODELS) @app.route('/predict/<hashno>', methods=['GET', 'POST']) def predict(hashno): print(PREDICTORS) if flask.request.method == 'POST': data = flask.request.data predictions = PREDICTORS[hashno].predict(np.array(data['data'])) return {'deployment no': hashno, 'predictions': [int(p) for p in predictions]} return {'data': [[5.6, 3.0, 4.5, 1.5], [5.6, 3.0, 4.5, 1.5]]} @app.route('/deployment/stop/<deployment_no>', methods=['GET']) def stop_deployment(deployment_no): global PREDICTORS ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for (idx, d) in enumerate(ALL_DEPLOYED_MODELS): if d['model_deployment_number'] == deployment_no: print('here here') ALL_DEPLOYED_MODELS[idx]['status'] = 'stopped' yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) PREDICTORS = {i: j for (i, j) in PREDICTORS.items() if i != deployment_no} return {'status': 200} @app.route('/deployment/start/<deployment_no>', methods=['GET']) def start_deployment(deployment_no): global PREDICTORS ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for (idx, d) in enumerate(ALL_DEPLOYED_MODELS): if d['model_deployment_number'] == deployment_no: ALL_DEPLOYED_MODELS[idx]['status'] = 'running' PREDICTORS[deployment_no] = _sklearn_prediction.Deployment(d['model_path']) yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) return {'status': 200} def start_ui(host=None, port=None, debug=False): \"\"\"Implemet logic for try catch\"\"\" ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for i in ALL_DEPLOYED_MODELS: deployed = _sklearn_prediction.Deployment(i['model_path']) PREDICTORS[i['model_deployment_number']] = deployed if host == None and port == None: app.run(debug=debug) elif host == None: app.run(port=port, debug=debug) elif port == None: app.run(host=host, debug=debug) else: app.run(host=host, port=port, debug=debug) if __name__ == '__main__': app.run() pymlpipe/utils/yamlio import yaml import os def read_yaml(path): if not os.path.exists(path): return [] with open(path) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) return fulllist def write_to_yaml(path, info): with open(os.path.join(path), 'w') as file: documents = yaml.dump(info, file) pymlpipe/utils/database import os MODEL_FOLDER_NAME = 'modelrun' def create_folder(folder_path, name=None): \"\"\"_summary_:create a folder for storing model information Returns: str: path for storing model details \"\"\" folder = MODEL_FOLDER_NAME if name != None: folder = name path = os.path.join(folder_path, folder) if not os.path.exists(path): os.mkdir(path) return path pymlpipe/utils/ init __version__ = '0.1.0' __author__ = 'indresh bhattacharya' pymlpipe/utils/getschema import pandas as pd def schema_(data): \"\"\"_summary_: Generate schema object for a dataframe Args: data (Pandas DataFrame): Pandas Artifact Returns: dict: with column schema \"\"\" schema = {} details = [] for col in data: schema[col] = {'min': float(data[col].min()), 'max': float(data[col].max()), 'std': float(data[col].std()), 'variance': float(data[col].var()), 'mean': float(data[col].mean()), 'median': float(data[col].median()), 'data type': str(data[col].dtype), 'unique_values': int(len(data[col].unique())), '25th percentile': str(data[col].quantile(0.25)), '50% percentile': str(data[col].quantile(0.5)), '75% percentile': str(data[col].quantile(0.75))} if len(details) == 0: details = list(schema[col].keys()) return (schema, details) pymlpipe/utils/_sklearn_prediction import pickle class Deployment: def __init__(self, model_path): self.model_path = model_path self.model = pickle.load(open(self.model_path, 'rb')) def predict(self, data): return self.model.predict(data)","title":"API"},{"location":"code/#source-code-index","text":"","title":"Source Code Index"},{"location":"code/#pymlpipetabular","text":"import os from pymlpipe.utils.database import create_folder from pymlpipe.utils.getschema import schema_ import uuid import yaml from contextlib import contextmanager import pandas as pd import shutil import pickle import sklearn import datetime class Context_Manager: \"\"\"_summary_: Context Manager for with statement 1. creates folders and subfolders 2. creates runid for a run instance \"\"\" def __init__(self, name, feature_store, run_id=None): super(PyMLPipe) if run_id == None: self.runid = str(uuid.uuid4()) else: self.runid = run_id self.name = name self.feature_store = feature_store self.exp_path = os.path.join(self.feature_store, self.name, self.runid) self.folders = {'artifacts': os.path.join(self.exp_path, 'artifacts'), 'metrics': os.path.join(self.exp_path, 'metrics'), 'models': os.path.join(self.exp_path, 'models'), 'params': os.path.join(self.exp_path, 'params')} self.info_dict = [] def get_path(self): \"\"\"_summary_ Returns: _type_: _description_ \"\"\" return self.exp_path def structure(self): \"\"\"_summary_ Returns: _type_: _description_ \"\"\" self.exp_path = create_folder(self.feature_store, self.name) self.exp_path = create_folder(self.exp_path, self.runid) self._create_all_folders(self.exp_path) return self.exp_path def _create_all_folders(self, exp_path): \"\"\"_summary_ Args: exp_path (_type_): _description_ \"\"\" for i in self.folders: create_folder(exp_path, i) def write_to_yaml(self, info): with open(os.path.join(self.exp_path, 'info.yaml'), 'w') as file: documents = yaml.dump(info, file) class PyMLPipe: def __init__(self): self.feature_store = create_folder(os.getcwd()) self.experiment_name = '0' self.folders = None self.experiment_path = None self.info = {} self.info['tags'] = [] self.info['metrics'] = {} self.info['params'] = {} self.info['artifact'] = [] self.info['model'] = {} self.info['artifact_schema'] = [] def __reset__(self): self.feature_store = create_folder(os.getcwd()) self.folders = None self.experiment_path = None self.info['tags'] = [] self.info['metrics'] = {} self.info['params'] = {} self.info['artifact'] = [] self.info['model'] = {} self.info['artifact_schema'] = [] @contextmanager def run(self, experiment_name=None, runid=None): \"\"\"_summary_: start a context manager for with statement 1. When run is started it will create a. RUN ID b. EXPERIMENT ID c. FOLDERS for storing the details Args: experiment_name (str, optional): gives a experiment name. Defaults to None. runid (str, optional): gives a runid. Defaults to None. Returns: class context_run(object): object for the context manager \"\"\" if experiment_name != None: self.experiment_name = experiment_name r = Context_Manager(self.experiment_name, self.feature_store, runid) self._write_info_run(self.experiment_name, r.runid) r.structure() self.context_manager = r self.scikit_learn = ScikitLearn(self.context_manager.folders) yield r self.info['execution_time'] = str(datetime.datetime.now()).split('.')[0] if self.scikit_learn.registered: self.info['model'] = {'model_name': self.scikit_learn.model_name, 'model_path': self.scikit_learn.model_path, 'model_params': self.scikit_learn.model_params, 'model_class': self.scikit_learn.model_class, 'model_type': self.scikit_learn.model_type, 'model_tags': self.scikit_learn.model_tags, 'registered': self.scikit_learn.registered} self.context_manager.write_to_yaml(self.info) self.__reset__() def set_experiment(self, name): \"\"\"_summary_: sets the experiment name Args: name (str): name of the experiment \"\"\" self.experiment_name = name exp_path = create_folder(self.feature_store, self.experiment_name) self._write_info_experiment(name, exp_path) def set_tag(self, tag_value): \"\"\"_summary_: sets a tag for a perticular run Args: name (str or int or float): tag name Raises: TypeError: Supported type 'str','int','float' \"\"\" if isinstance(tag_value, dict) or isinstance(tag_value, list) or isinstance(tag_value, set): raise TypeError(\"unsupported type, Expected 'str','int','float' got \" + str(type(tag_value))) self.info['tags'].append(tag_value) def set_tags(self, tag_dict: list): \"\"\"_summary_:sets N no of tags for a perticular run Args: tag_dict (list): tag names in list format Raises: TypeError: Expected 'list' \"\"\" if isinstance(tag_dict, list): self.info['tags'].extend(tag_dict) else: raise TypeError(\"unsupported type, Expected 'list' got \" + str(type(tag_dict))) def get_tags(self): \"\"\"_summary_: get all the tags that are associated with the run Returns: list: tags that are associated with the run \"\"\" return self.info['tags'] def set_version(self, version): \"\"\"_summary_:sets version number for the perticular run Args: version (str or int or float): version number Raises: TypeError: Expected 'str','int','float' \"\"\" if isinstance(version, dict) or isinstance(version, list) or isinstance(version, set): raise TypeError(\"unsupported type, Expected 'str','int','float' got \" + str(type(tag_dict))) self.info['version'] = version def get_version(self): \"\"\"_summary_:get the version number associated with the run Returns: _type_: version number \"\"\" return self.info['version'] def log_metrics(self, metric_dict: dict): \"\"\"_summary_: log metrics for the model run Args: metric_dict (dict): key value pair with metric name and metric value Raises: TypeError: Expected 'dict' \"\"\" if isinstance(metric_dict, dict): self.info['metrics'].update({i: float('{0:.2f}'.format(j)) for (i, j) in metric_dict.items()}) else: raise TypeError(\"unsupported type, Expected 'dict' got \" + str(type(metric_dict))) def log_metric(self, metric_name, metric_value): \"\"\"_summary_: log single metric for the model run Args: metric_name (str): name of the metric metric_value (int or float): value of the metric Raises: TypeError: metric_name expected to be str TypeError: metric_value expected to be int or float \"\"\" mv = None if not isinstance(metric_value, int) and (not isinstance(metric_value, float)): raise TypeError(\"unsupported type, 'metric_value' Expected 'int','float' got \" + str(type(metric_value))) if not isinstance(metric_name, str): raise TypeError(\"unsupported type, 'metric_value' Expected 'str' got \" + str(type(metric_name))) self.info['metrics'][metric_name] = float('{0:.2f}'.format(metric_value)) def log_params(self, param_dict: dict): \"\"\"_summary_: log parameters for the model run Args: param_dict (dict): key value pair with parameter name and parameter value Raises: TypeError: Expected 'dict' \"\"\" if isinstance(param_dict, dict): self.info['params'].update(param_dict) else: raise TypeError(\"unsupported type, Expected 'dict' got \" + str(type(metric_dict))) def log_param(self, param_name, param_value): \"\"\"_summary_:log single parameter for the model run Args: param_name (str): _description_ param_value (int or float or str): _description_ Raises: TypeError: param_name Expected 'str' TypeError: param_value Expected 'int','float','str' \"\"\" mv = None if not isinstance(param_value, int) and (not isinstance(param_value, float)) and (not isinstance(param_value, str)): raise TypeError(\"unsupported type, 'param_value' Expected 'int','float','str' got \" + str(type(metric_value))) if not isinstance(param_name, str): raise TypeError(\"unsupported type, 'param_name' Expected 'str' got \" + str(type(metric_name))) self.info['params'][param_name] = param_value def register_artifact(self, artifact_name, artifact, artifact_type='training'): \"\"\"_summary_: Save Artifact as part of data verion control Args: artifact_name (str): name of the artifact artifact (pandas DataFrame): pandas DataFrame object with the data artifact_type (str, optional): Defaults to \"training\". artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: Expected DataFrame object ValueError: artifact_name should have a string value \"\"\" if not isinstance(artifact, pd.DataFrame): raise TypeError(\"Please provide DataFrame in 'artifact'\") if artifact_name == '' or artifact_name == None: raise ValueError(\"Please provide a name in 'artifact_name' which is not '' or None\") path = os.path.join(self.context_manager.folders['artifacts'], artifact_name) dataschema = artifact.describe(include='all') artifact.to_csv(path, index=False) self.info['artifact'].append({'name': artifact_name, 'path': path, 'tag': artifact_type}) (schema_data, schema_details) = schema_(artifact) self.info['artifact_schema'].append({'name': artifact_name, 'schema': schema_data, 'details': schema_details}) def register_artifact_with_path(self, artifact, artifact_type='training'): \"\"\"_summary_ Args: artifact (str): path of the artifact artifact_type (str, optional): _description_. Defaults to \"training\".artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: artifact path should be str ValueError: artifact path should be correct \"\"\" if not isinstance(artifact, str): raise TypeError('Please provide full path of artifact') if not os.path.exists(artifact): raise ValueError('Please provide correct path of artifact') shutil.copy(artifact, self.context_manager.folders['artifacts']) path = os.path.join(self.context_manager.folders['artifacts'], os.path.basename(artifact)) self.info['artifact'].append({'name': os.path.basename(path), 'path': path, 'tag': artifact_type}) filename = os.path.basename(artifact) if filename.endswith('.csv'): artifact = pd.read_csv(path) elif filename.endswith('.xlxs'): artifact = pd.read_excel(path) elif filename.endswith('.parquet'): artifact = pd.read_parquet(path) else: print('Error: Unknown file type cannot generate Schema!!!!') return (schema_data, schema_details) = schema_(artifact) self.info['artifact_schema'].append({'name': filename, 'schema': schema_data, 'details': schema_details}) def get_info(self): \"\"\"_summary_: get the whole run details Returns: dict: information about the whole run \"\"\" return self.info def get_artifact(self): \"\"\"_summary_: get the artifact details Returns: dict: returns the artifact detail \"\"\" return self.info['artifact'] def _write_info_experiment(self, experiment_name, path): \"\"\"_summary_: writes to the experiment schema Args: experiment_name (str): name of the experiment path (str): path to save the run details \"\"\" fulllist = {} if os.path.exists(os.path.join(self.feature_store, 'experiment.yaml')): with open(os.path.join(self.feature_store, 'experiment.yaml')) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) if experiment_name not in fulllist: fulllist[experiment_name] = {'experiment_path': path, 'runs': [], 'execution_time': str(datetime.datetime.now()).split('.')[0]} else: fulllist[experiment_name]['execution_time'] = str(datetime.datetime.now()).split('.')[0] else: fulllist[experiment_name] = {'experiment_path': path, 'runs': [], 'execution_time': str(datetime.datetime.now()).split('.')[0]} with open(os.path.join(self.feature_store, 'experiment.yaml'), 'w') as file: documents = yaml.dump(fulllist, file) def _write_info_run(self, experiment_name, run_id): \"\"\"_summary_:writes to the run schema Args: experiment_name (str): name of the experiment run_id (str): ID for the running instance \"\"\" fulllist = {} with open(os.path.join(self.feature_store, 'experiment.yaml')) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) fulllist[experiment_name]['runs'].append(run_id) with open(os.path.join(self.feature_store, 'experiment.yaml'), 'w') as file: documents = yaml.dump(fulllist, file) def set_uri(self): pass class ScikitLearn: def __init__(self, folders): self.folders = folders self.model_name = '' self.model_path = '' self.model_class = '' self.model_type = '' self.model_params = {} self.model_tags = {} self.registered = False def register_model(self, model_name, model): if 'sklearn' in str(type(model)): pickle.dump(model, open(os.path.join(self.folders['models'], model_name + '.pkl'), 'wb')) else: raise TypeError('Error:Expected ScikitLearn Module!!!!') self.model_name = model_name self.model_path = os.path.join(self.folders['models'], model_name + '.pkl') self.model_class = type(model).__name__ self.model_params = model.get_params() self.model_tags = {tag: str(value) for (tag, value) in model._get_tags().items()} self.model_type = 'scikit-learn' self.registered = True","title":"pymlpipe/tabular"},{"location":"code/#pymlpipeinit","text":"","title":"pymlpipe/init"},{"location":"code/#pymlpipepymlpipeui","text":"import flask import os from pymlpipe.utils import yamlio from pymlpipe.utils import _sklearn_prediction from flask_api import FlaskAPI import numpy as np import json import uuid app = FlaskAPI(__name__) BASE_DIR = os.getcwd() MODEL_FOLDER_NAME = 'modelrun' MODEL_DIR = os.path.join(BASE_DIR, MODEL_FOLDER_NAME) EXPERIMENT_FILE = 'experiment.yaml' DEPLOYMENT_FILE = 'deployment.yaml' PREDICTORS = {} app.secret_key = 'PYMLPIPE_SEC_KEY' @app.route('/') def index(): \"\"\" if \"status\" in flask.request.args: if flask.request.args[\"status\"]==\"501\": deploy_status=False \"\"\" metric_filters = {} tag_filters = [] if len(flask.request.args): if 'metrics' in flask.request.args: metric_filters[flask.request.args['metrics']] = flask.request.args['metricsfilter'] elif 'tags' in flask.request.args: tag_filters = flask.request.args['tags'].split(',') experiment_lists = yamlio.read_yaml(os.path.join(MODEL_DIR, EXPERIMENT_FILE)) if len(experiment_lists) == 0: return flask.render_template('index.html', runs=[], run_details={}, metrics=[], current_experiment=None) info = {} metrics = [] exp_wise_metrics = {} tags = [] error = '' for (experiment, run_data) in experiment_lists.items(): for run_id in run_data['runs']: print(run_data['experiment_path'], run_id, 'info.yaml') run_folder = os.path.join(run_data['experiment_path'], run_id, 'info.yaml') run_details = yamlio.read_yaml(run_folder) info[run_id] = run_details if 'tags' in run_details: tags.extend(run_details['tags']) if 'metrics' in run_details: metrics.extend(list(run_details['metrics'].keys())) exp_wise_metrics[experiment] = list(run_details['metrics'].keys()) info = {run: info[run] for run in info if len(info[run]) > 0} if len(metric_filters) > 0: newinfo = {} for (run_id, details) in info.items(): for mfilter in metric_filters: if mfilter in details['metrics']: fv = details['metrics'][mfilter] try: if eval(str(fv) + metric_filters[mfilter]): newinfo[run_id] = details except Exception as e: error = e else: newinfo[run_id] = details info = newinfo elif len(tag_filters) > 0: newinfo = {} for (run_id, details) in info.items(): if len(set(tag_filters).intersection(set(details['tags']))) > 0: newinfo[run_id] = details info = newinfo exp_names = list(experiment_lists.keys()) return flask.render_template('index.html', runs=experiment_lists, run_details=info, metrics=list(set(metrics)), current_experiment=exp_names, tags=list(set(tags)), exp_wise_metrics=exp_wise_metrics, error=error) @app.route('/run/<run_id>/') def runpage(run_id): deploy_status = True if 'status' in flask.request.args: if flask.request.args['status'] == '501': deploy_status = False (experiments, run_id) = run_id.split('@') experiment_lists = yamlio.read_yaml(os.path.join(MODEL_DIR, EXPERIMENT_FILE)) run_details = yamlio.read_yaml(os.path.join(MODEL_DIR, experiments, run_id, 'info.yaml')) expertiment_details = {'RUN_ID': run_id, 'EXPERIMENT NAME': experiments, 'EXECUTION DATE TIME': run_details['execution_time']} if 'tags' in run_details: expertiment_details['TAGS'] = run_details['tags'] else: expertiment_details['TAGS'] = '-' if 'version' in run_details: expertiment_details['VERSION'] = run_details['version'] else: expertiment_details['VERSION'] = '-' return flask.render_template('run.html', run_id=run_id, experiments=experiments, expertiment_details=expertiment_details, artifact_details=run_details['artifact'], metrics_details=run_details['metrics'], model_details=run_details['model'], param_details=run_details['params'], schema_details=run_details['artifact_schema'], is_deployed=True if 'model_path' in run_details['model'] else False, deploy_status=deploy_status) @app.route('/download_artifact/<uid>') def download_artifact(uid): (experiments, run_id, filename) = uid.split('@') return flask.send_from_directory(os.path.join(MODEL_DIR, experiments, run_id, 'artifacts'), filename, as_attachment=True) @app.route('/download_model/<uid>') def download_model(uid): (experiments, run_id, filename, model_type) = uid.split('@') if model_type == 'scikit-learn': filename = filename + '.pkl' return flask.send_from_directory(os.path.join(MODEL_DIR, experiments, run_id, 'models'), filename, as_attachment=True) @app.route('/deployments/<run_id>/') def deployments(run_id): (experiments, runid) = run_id.split('@') run_details = yamlio.read_yaml(os.path.join(MODEL_DIR, experiments, runid, 'info.yaml')) deployed = _sklearn_prediction.Deployment(run_details['model']['model_path']) print(uuid.NAMESPACE_DNS) run_hash = str(uuid.uuid3(uuid.NAMESPACE_DNS, run_id)).replace('-', '')[:16] if run_hash not in PREDICTORS: PREDICTORS[run_hash] = deployed ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) ALL_DEPLOYED_MODELS.append({'run_id': runid, 'experiment_id': experiments, 'model_path': run_details['model']['model_path'], 'model_deployment_number': run_hash, 'model_url': '/predict/' + run_hash, 'status': 'running'}) yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) return flask.redirect(flask.url_for('show_deployments')) return flask.redirect('/run/' + run_id + '?status=501') @app.route('/show_deployments/') def show_deployments(): ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) return flask.render_template('deployments.html', ALL_DEPLOYED_MODELS=ALL_DEPLOYED_MODELS) @app.route('/predict/<hashno>', methods=['GET', 'POST']) def predict(hashno): print(PREDICTORS) if flask.request.method == 'POST': data = flask.request.data predictions = PREDICTORS[hashno].predict(np.array(data['data'])) return {'deployment no': hashno, 'predictions': [int(p) for p in predictions]} return {'data': [[5.6, 3.0, 4.5, 1.5], [5.6, 3.0, 4.5, 1.5]]} @app.route('/deployment/stop/<deployment_no>', methods=['GET']) def stop_deployment(deployment_no): global PREDICTORS ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for (idx, d) in enumerate(ALL_DEPLOYED_MODELS): if d['model_deployment_number'] == deployment_no: print('here here') ALL_DEPLOYED_MODELS[idx]['status'] = 'stopped' yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) PREDICTORS = {i: j for (i, j) in PREDICTORS.items() if i != deployment_no} return {'status': 200} @app.route('/deployment/start/<deployment_no>', methods=['GET']) def start_deployment(deployment_no): global PREDICTORS ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for (idx, d) in enumerate(ALL_DEPLOYED_MODELS): if d['model_deployment_number'] == deployment_no: ALL_DEPLOYED_MODELS[idx]['status'] = 'running' PREDICTORS[deployment_no] = _sklearn_prediction.Deployment(d['model_path']) yamlio.write_to_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE), ALL_DEPLOYED_MODELS) return {'status': 200} def start_ui(host=None, port=None, debug=False): \"\"\"Implemet logic for try catch\"\"\" ALL_DEPLOYED_MODELS = yamlio.read_yaml(os.path.join(MODEL_DIR, DEPLOYMENT_FILE)) for i in ALL_DEPLOYED_MODELS: deployed = _sklearn_prediction.Deployment(i['model_path']) PREDICTORS[i['model_deployment_number']] = deployed if host == None and port == None: app.run(debug=debug) elif host == None: app.run(port=port, debug=debug) elif port == None: app.run(host=host, debug=debug) else: app.run(host=host, port=port, debug=debug) if __name__ == '__main__': app.run()","title":"pymlpipe/pymlpipeUI"},{"location":"code/#pymlpipeutilsyamlio","text":"import yaml import os def read_yaml(path): if not os.path.exists(path): return [] with open(path) as file: fulllist = yaml.load(file, Loader=yaml.FullLoader) return fulllist def write_to_yaml(path, info): with open(os.path.join(path), 'w') as file: documents = yaml.dump(info, file)","title":"pymlpipe/utils/yamlio"},{"location":"code/#pymlpipeutilsdatabase","text":"import os MODEL_FOLDER_NAME = 'modelrun' def create_folder(folder_path, name=None): \"\"\"_summary_:create a folder for storing model information Returns: str: path for storing model details \"\"\" folder = MODEL_FOLDER_NAME if name != None: folder = name path = os.path.join(folder_path, folder) if not os.path.exists(path): os.mkdir(path) return path","title":"pymlpipe/utils/database"},{"location":"code/#pymlpipeutilsinit","text":"__version__ = '0.1.0' __author__ = 'indresh bhattacharya'","title":"pymlpipe/utils/init"},{"location":"code/#pymlpipeutilsgetschema","text":"import pandas as pd def schema_(data): \"\"\"_summary_: Generate schema object for a dataframe Args: data (Pandas DataFrame): Pandas Artifact Returns: dict: with column schema \"\"\" schema = {} details = [] for col in data: schema[col] = {'min': float(data[col].min()), 'max': float(data[col].max()), 'std': float(data[col].std()), 'variance': float(data[col].var()), 'mean': float(data[col].mean()), 'median': float(data[col].median()), 'data type': str(data[col].dtype), 'unique_values': int(len(data[col].unique())), '25th percentile': str(data[col].quantile(0.25)), '50% percentile': str(data[col].quantile(0.5)), '75% percentile': str(data[col].quantile(0.75))} if len(details) == 0: details = list(schema[col].keys()) return (schema, details)","title":"pymlpipe/utils/getschema"},{"location":"code/#pymlpipeutils_sklearn_prediction","text":"import pickle class Deployment: def __init__(self, model_path): self.model_path = model_path self.model = pickle.load(open(self.model_path, 'rb')) def predict(self, data): return self.model.predict(data)","title":"pymlpipe/utils/_sklearn_prediction"},{"location":"featurelist/","text":"Feature List Framework support [x] Scikit-learn Support [x] Pytorch Support [ ] TesorFlow Support Functionality [ ] Data Drift Detection [ ] XAI -Explainable AI [ ] Data Pipelines with Dags [ ] Model Export and Containrization Data Types [X] Tabular [ ] Natural Language [ ] Computer Vision","title":"Feature"},{"location":"featurelist/#feature-list","text":"","title":"Feature List"},{"location":"featurelist/#framework-support","text":"[x] Scikit-learn Support [x] Pytorch Support [ ] TesorFlow Support","title":"Framework support"},{"location":"featurelist/#functionality","text":"[ ] Data Drift Detection [ ] XAI -Explainable AI [ ] Data Pipelines with Dags [ ] Model Export and Containrization","title":"Functionality"},{"location":"featurelist/#data-types","text":"[X] Tabular [ ] Natural Language [ ] Computer Vision","title":"Data Types"},{"location":"package-index/","text":"Index pymlpipe/tabular Class-> Context_Manager: Module-> init (self,name,feature_store,run_id): Module-> get_path(self): _summary_ Returns: _type_: _description_ Module-> structure(self): _summary_ Returns: _type_: _description_ Module-> _create_all_folders(self,exp_path): _summary_ Args: exp_path (_type_): _description_ Module-> write_to_yaml(self,info): Class-> PyMLPipe: Module-> init (self): Module-> reset (self): Module-> run(self,experiment_name,runid): _summary_: start a context manager for with statement 1. When run is started it will create a. RUN ID b. EXPERIMENT ID c. FOLDERS for storing the details Args: experiment_name (str, optional): gives a experiment name. Defaults to None. runid (str, optional): gives a runid. Defaults to None. Returns: class context_run(object): object for the context manager Module-> set_experiment(self,name): _summary_: sets the experiment name Args: name (str): name of the experiment Module-> set_tag(self,tag_value): _summary_: sets a tag for a perticular run Args: name (str or int or float): tag name Raises: TypeError: Supported type 'str','int','float' Module-> set_tags(self,tag_dict): _summary_:sets N no of tags for a perticular run Args: tag_dict (list): tag names in list format Raises: TypeError: Expected 'list' Module-> get_tags(self): _summary_: get all the tags that are associated with the run Returns: list: tags that are associated with the run Module-> set_version(self,version): _summary_:sets version number for the perticular run Args: version (str or int or float): version number Raises: TypeError: Expected 'str','int','float' Module-> get_version(self): _summary_:get the version number associated with the run Returns: _type_: version number Module-> log_metrics(self,metric_dict): _summary_: log metrics for the model run Args: metric_dict (dict): key value pair with metric name and metric value Raises: TypeError: Expected 'dict' Module-> log_metric(self,metric_name,metric_value): _summary_: log single metric for the model run Args: metric_name (str): name of the metric metric_value (int or float): value of the metric Raises: TypeError: metric_name expected to be str TypeError: metric_value expected to be int or float Module-> log_params(self,param_dict): _summary_: log parameters for the model run Args: param_dict (dict): key value pair with parameter name and parameter value Raises: TypeError: Expected 'dict' Module-> log_param(self,param_name,param_value): _summary_:log single parameter for the model run Args: param_name (str): _description_ param_value (int or float or str): _description_ Raises: TypeError: param_name Expected 'str' TypeError: param_value Expected 'int','float','str' Module-> register_artifact(self,artifact_name,artifact,artifact_type): _summary_: Save Artifact as part of data verion control Args: artifact_name (str): name of the artifact artifact (pandas DataFrame): pandas DataFrame object with the data artifact_type (str, optional): Defaults to \"training\". artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: Expected DataFrame object ValueError: artifact_name should have a string value Module-> register_artifact_with_path(self,artifact,artifact_type): _summary_ Args: artifact (str): path of the artifact artifact_type (str, optional): _description_. Defaults to \"training\".artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: artifact path should be str ValueError: artifact path should be correct Module-> get_info(self): _summary_: get the whole run details Returns: dict: information about the whole run Module-> get_artifact(self): _summary_: get the artifact details Returns: dict: returns the artifact detail Module-> _write_info_experiment(self,experiment_name,path): _summary_: writes to the experiment schema Args: experiment_name (str): name of the experiment path (str): path to save the run details Module-> _write_info_run(self,experiment_name,run_id): _summary_:writes to the run schema Args: experiment_name (str): name of the experiment run_id (str): ID for the running instance Module-> set_uri(self): Class-> ScikitLearn: Module-> init (self,folders): Module-> register_model(self,model_name,model): pymlpipe/ init pymlpipe/pymlpipeUI Module-> index() if \"status\" in flask.request.args: if flask.request.args[\"status\"]==\"501\": deploy_status=False Module-> runpage(run_id) Module-> download_artifact(uid) Module-> download_model(uid) Module-> deployments(run_id) Module-> show_deployments() Module-> predict(hashno) Module-> stop_deployment(deployment_no) Module-> start_deployment(deployment_no) Module-> start_ui(host,port,debug) Implemet logic for try catch pymlpipe/utils/yamlio Module-> read_yaml(path) Module-> write_to_yaml(path,info) pymlpipe/utils/database Module-> create_folder(folder_path,name) _summary_:create a folder for storing model information Returns: str: path for storing model details pymlpipe/utils/ init pymlpipe/utils/getschema Module-> schema_(data) _summary_: Generate schema object for a dataframe Args: data (Pandas DataFrame): Pandas Artifact Returns: dict: with column schema pymlpipe/utils/_sklearn_prediction Class-> Deployment: Module-> init (self,model_path): Module-> predict(self,data):","title":"Usage"},{"location":"package-index/#index","text":"","title":"Index"},{"location":"package-index/#pymlpipetabular","text":"","title":"pymlpipe/tabular"},{"location":"package-index/#class-context_manager","text":"Module-> init (self,name,feature_store,run_id): Module-> get_path(self): _summary_ Returns: _type_: _description_ Module-> structure(self): _summary_ Returns: _type_: _description_ Module-> _create_all_folders(self,exp_path): _summary_ Args: exp_path (_type_): _description_ Module-> write_to_yaml(self,info):","title":"Class-&gt; Context_Manager:"},{"location":"package-index/#class-pymlpipe","text":"Module-> init (self): Module-> reset (self): Module-> run(self,experiment_name,runid): _summary_: start a context manager for with statement 1. When run is started it will create a. RUN ID b. EXPERIMENT ID c. FOLDERS for storing the details Args: experiment_name (str, optional): gives a experiment name. Defaults to None. runid (str, optional): gives a runid. Defaults to None. Returns: class context_run(object): object for the context manager Module-> set_experiment(self,name): _summary_: sets the experiment name Args: name (str): name of the experiment Module-> set_tag(self,tag_value): _summary_: sets a tag for a perticular run Args: name (str or int or float): tag name Raises: TypeError: Supported type 'str','int','float' Module-> set_tags(self,tag_dict): _summary_:sets N no of tags for a perticular run Args: tag_dict (list): tag names in list format Raises: TypeError: Expected 'list' Module-> get_tags(self): _summary_: get all the tags that are associated with the run Returns: list: tags that are associated with the run Module-> set_version(self,version): _summary_:sets version number for the perticular run Args: version (str or int or float): version number Raises: TypeError: Expected 'str','int','float' Module-> get_version(self): _summary_:get the version number associated with the run Returns: _type_: version number Module-> log_metrics(self,metric_dict): _summary_: log metrics for the model run Args: metric_dict (dict): key value pair with metric name and metric value Raises: TypeError: Expected 'dict' Module-> log_metric(self,metric_name,metric_value): _summary_: log single metric for the model run Args: metric_name (str): name of the metric metric_value (int or float): value of the metric Raises: TypeError: metric_name expected to be str TypeError: metric_value expected to be int or float Module-> log_params(self,param_dict): _summary_: log parameters for the model run Args: param_dict (dict): key value pair with parameter name and parameter value Raises: TypeError: Expected 'dict' Module-> log_param(self,param_name,param_value): _summary_:log single parameter for the model run Args: param_name (str): _description_ param_value (int or float or str): _description_ Raises: TypeError: param_name Expected 'str' TypeError: param_value Expected 'int','float','str' Module-> register_artifact(self,artifact_name,artifact,artifact_type): _summary_: Save Artifact as part of data verion control Args: artifact_name (str): name of the artifact artifact (pandas DataFrame): pandas DataFrame object with the data artifact_type (str, optional): Defaults to \"training\". artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: Expected DataFrame object ValueError: artifact_name should have a string value Module-> register_artifact_with_path(self,artifact,artifact_type): _summary_ Args: artifact (str): path of the artifact artifact_type (str, optional): _description_. Defaults to \"training\".artifact_type can be [training,testing,validation,dev,prod] Raises: TypeError: artifact path should be str ValueError: artifact path should be correct Module-> get_info(self): _summary_: get the whole run details Returns: dict: information about the whole run Module-> get_artifact(self): _summary_: get the artifact details Returns: dict: returns the artifact detail Module-> _write_info_experiment(self,experiment_name,path): _summary_: writes to the experiment schema Args: experiment_name (str): name of the experiment path (str): path to save the run details Module-> _write_info_run(self,experiment_name,run_id): _summary_:writes to the run schema Args: experiment_name (str): name of the experiment run_id (str): ID for the running instance Module-> set_uri(self):","title":"Class-&gt; PyMLPipe:"},{"location":"package-index/#class-scikitlearn","text":"Module-> init (self,folders): Module-> register_model(self,model_name,model):","title":"Class-&gt; ScikitLearn:"},{"location":"package-index/#pymlpipeinit","text":"","title":"pymlpipe/init"},{"location":"package-index/#pymlpipepymlpipeui","text":"Module-> index() if \"status\" in flask.request.args: if flask.request.args[\"status\"]==\"501\": deploy_status=False Module-> runpage(run_id) Module-> download_artifact(uid) Module-> download_model(uid) Module-> deployments(run_id) Module-> show_deployments() Module-> predict(hashno) Module-> stop_deployment(deployment_no) Module-> start_deployment(deployment_no) Module-> start_ui(host,port,debug) Implemet logic for try catch","title":"pymlpipe/pymlpipeUI"},{"location":"package-index/#pymlpipeutilsyamlio","text":"Module-> read_yaml(path) Module-> write_to_yaml(path,info)","title":"pymlpipe/utils/yamlio"},{"location":"package-index/#pymlpipeutilsdatabase","text":"Module-> create_folder(folder_path,name) _summary_:create a folder for storing model information Returns: str: path for storing model details","title":"pymlpipe/utils/database"},{"location":"package-index/#pymlpipeutilsinit","text":"","title":"pymlpipe/utils/init"},{"location":"package-index/#pymlpipeutilsgetschema","text":"Module-> schema_(data) _summary_: Generate schema object for a dataframe Args: data (Pandas DataFrame): Pandas Artifact Returns: dict: with column schema","title":"pymlpipe/utils/getschema"},{"location":"package-index/#pymlpipeutils_sklearn_prediction","text":"","title":"pymlpipe/utils/_sklearn_prediction"},{"location":"package-index/#class-deployment","text":"Module-> init (self,model_path): Module-> predict(self,data):","title":"Class-&gt; Deployment:"}]}